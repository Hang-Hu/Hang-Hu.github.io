---
layout: post
author: Hang Hu
categories: python
tags: Blog Python 
cover: 
---
## RDD

### I/O

#### Read

```python
rdd = sc.textFile(filepath)
```


```python
text = sc.textFile(filepath)
```


#### Write


```python
rdd.saveAsTextFile(filepath)
```


```python
sc.parallelize(result_list).saveAsTextFile(output)
```


### list to RDD: `sc.parallelize`


```python
# (key, value), which in detail is (subreddit_name, (visit_count, bytes transferred))

reddit = sc.parallelize([('xkcd', (12, 2121)), ('xkcd', (213, 42323))])
reddit = reddit.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) # reduce by subreddit_name

```


### return more: `yield`, and flat to list: `rdd.flatMap`


```python
def words_once(line):
    wordsep = re.compile(r'[%s\s]+' % re.escape(string.punctuation))
    for w in wordsep.split(line):
        yield (w.lower(), 1)


text = sc.textFile(inputs)
words = text.flatMap(words_once)
```


### use another rdd in map: `rdd.join(another_rdd)` or `sc.broadcast(dict(another_rdd.collect()))`


#### join


```python
rdd.join(another_rdd)
```


```python
def iterate_once(tuple):
    node, ((source, distance), outgoing_nodes) = tuple
    yield (node, (source, distance)) # currently-known path

    # new paths found by going one more step in the graph

    for out in outgoing_nodes:
        yield (out, (node, distance+1))

known_paths = known_paths.join(graph_edges) # (node, ((source, distance), list of outgoing nodes))

known_paths = known_paths.flatMap(iterate_once)
```


#### broadcast


```python
sc.broadcast(dict(another_rdd.collect()))
```


```python
def calc_relative_score(average, comment):
    sub_ave = average.value[comment['subreddit']]
    return comment['score']/sub_ave

broadcast_average = sc.broadcast(dict(average.collect()))
result = commentdata.map(lambda x: (calc_relative_score(broadcast_average, x), x['author'])).sortBy(lambda x: x[0])
```


### dict like search: `rdd.lookup(key)`


```python
# if known_paths has row whose key is 4, then break

if known_paths.lookup(4):
    break
```


### cache


```
rdd = rdd.cache()
```

### remove first row

```
header = raw.take(1)[0]
data = raw.filter(lambda l: l != header)
```

### Check if RDD empty

```
if rec_rdd.isEmpty():
    return []
rec_df = spark.createDataFrame(rec_rdd)\
    .withColumn('user_id', functions.lit(user_id))\
    .withColumn('timestamp', functions.current_timestamp())
```


## Data Frame

### reduce strings

```
@functions.udf(returnType=types.StringType())
def to_name_str(namelist):
    return ', '.join(sorted(namelist))


res_df = res_df.groupby('orderkey', 'totalprice')\
        .agg(to_name_str(functions.collect_set('name')).alias('name'))
```

### list to dataframe pyspark

```
orderkeys_df = spark.createDataFrame(orderkeys, IntegerType())
```

### spark config

```
spark = SparkSession.builder.appName('TPCH Denormalize') \
    .config('spark.cassandra.connection.host', ','.join(cluster_seeds))\
    .config('spark.dynamicAllocation.maxExecutors', 16)\
    .getOrCreate()
```

### Get the value in one row one column dataframe

```
r = res.collect()[0][0]
```

### combine two dataframe

```
result = slope.union(intercept)

```

### RDD to DataFrame: createDataFrame

```
line_re = re.compile(r'^(\S+) - - \[(\S+) [+-]\d+\] \"[A-Z]+ (\S+) HTTP/\d\.\d\" \d+ (\d+)$')
logs = sc.textFile(input_dir) \
    .repartition(10) \
    .map(lambda x: line_re.split(x)) \
    .map(format_line) \
    .filter(lambda x: x is not None)
data_rows = logs.map(lambda x: Row(host=x[0], id=str(uuid.uuid4()), bytes=x[1], datetime=x[2], path=x[3]))
data = spark.createDataFrame(data_rows)
```

### Create dataframe from scratch

#### Multiple columns

```
test = spark.createDataFrame([(1, 1), (1, 2), (1, 3)], ["user_id", "business_id"])
```

#### Single Column

```
user_id = 1
user_df = spark.createDataFrame([user_id], types.IntegerType())
user_df = user_df.select(user_df['value'].alias('user_id'))
```

### Extract Rows inside one grid of dataframe using `.flatMap`

```
rec = ratings_model.recommendForUserSubset(user_df, 3).select('recommendations')
```

```
>>> rec.show()
+-------+--------------------+                                                  
|user_id|     recommendations|
+-------+--------------------+
|      1|[[16356, 9.150447...|
+-------+--------------------+
```

```
rec_rdd = rec.rdd\
    .flatMap(lambda x: x['recommendations'])\
    .map(lambda x: (x['business_id'], x['rating']))
```

### withColumn

```
data = data.withColumn('slope',
        (data['sum((x * y))']-data['sum(x)']*data['sum(y)']/data['sum(1)']) / (data['sum(POWER(x, 2))']-data['sum(x)']**2/data['sum(1)']))
```