---
layout: post
author: Hang Hu
categories: python
tags: Blog Python 
cover: 
---

## Skeleton for both RDD and DataFrame

```
import sys
assert sys.version_info >= (3, 5) # make sure we have Python 3.5+

from pyspark.sql import SparkSession, functions, types
spark = SparkSession.builder.appName('example code').getOrCreate()
assert spark.version >= '2.3' # make sure we have Spark 2.3+
spark.sparkContext.setLogLevel('WARN')
sc = spark.sparkContext

# add more functions as necessary

def main(inputs, output):
    # main logic starts here

if __name__ == '__main__':
    inputs = sys.argv[1]
    output = sys.argv[2]
    main(inputs, output)
```

## RDD

### Skeleton

```
Spark + RDDs
from pyspark import SparkConf, SparkContext
import sys
assert sys.version_info >= (3, 5) # make sure we have Python 3.5+

# add more functions as necessary

def main(inputs, output):
    # main logic starts here

if __name__ == '__main__':
    conf = SparkConf().setAppName('example code')
    sc = SparkContext(conf=conf)
    # sc.setLogLevel('WARN')
    assert sc.version >= '2.3'  # make sure we have Spark 2.3+
    inputs = sys.argv[1]
    output = sys.argv[2]
    main(inputs, output)
```

### I/O

#### Read

```python
rdd = sc.textFile(filepath)
```


```python
text = sc.textFile(filepath)
```


#### Write


```python
rdd.saveAsTextFile(filepath)
```


```python
sc.parallelize(result_list).saveAsTextFile(output)
```


### list to RDD: `sc.parallelize`


```python
# (key, value), which in detail is (subreddit_name, (visit_count, bytes transferred))

reddit = sc.parallelize([('xkcd', (12, 2121)), ('xkcd', (213, 42323))])
reddit = reddit.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) # reduce by subreddit_name

```


### return more: `yield`, and flat to list: `rdd.flatMap`


```python
def words_once(line):
    wordsep = re.compile(r'[%s\s]+' % re.escape(string.punctuation))
    for w in wordsep.split(line):
        yield (w.lower(), 1)


text = sc.textFile(inputs)
words = text.flatMap(words_once)
```


### use another rdd in map: `rdd.join(another_rdd)` or `sc.broadcast(dict(another_rdd.collect()))`


#### join


```python
rdd.join(another_rdd)
```


```python
def iterate_once(tuple):
    node, ((source, distance), outgoing_nodes) = tuple
    yield (node, (source, distance)) # currently-known path

    # new paths found by going one more step in the graph

    for out in outgoing_nodes:
        yield (out, (node, distance+1))

known_paths = known_paths.join(graph_edges) # (node, ((source, distance), list of outgoing nodes))

known_paths = known_paths.flatMap(iterate_once)
```


#### broadcast


```python
sc.broadcast(dict(another_rdd.collect()))
```


```python
def calc_relative_score(average, comment):
    sub_ave = average.value[comment['subreddit']]
    return comment['score']/sub_ave

broadcast_average = sc.broadcast(dict(average.collect()))
result = commentdata.map(lambda x: (calc_relative_score(broadcast_average, x), x['author'])).sortBy(lambda x: x[0])
```


### dict like search: `rdd.lookup(key)`


```python
# if known_paths has row whose key is 4, then break

if known_paths.lookup(4):
    break
```


### cache


```
rdd = rdd.cache()
```

### remove first row

```
header = raw.take(1)[0]
data = raw.filter(lambda l: l != header)
```

### remove rows with given condition

```
resultDF = resultDF.filter(resultDF.jaccard >= threshold)
```

### Check if RDD empty

```
if rec_rdd.isEmpty():
    return []
rec_df = spark.createDataFrame(rec_rdd)\
    .withColumn('user_id', functions.lit(user_id))\
    .withColumn('timestamp', functions.current_timestamp())
```


## Data Frame

### Skeleton

```
import sys
assert sys.version_info >= (3, 5) # make sure we have Python 3.5+

from pyspark.sql import SparkSession, functions, types
spark = SparkSession.builder.appName('example code').getOrCreate()
assert spark.version >= '2.3' # make sure we have Spark 2.3+
# spark.sparkContext.setLogLevel('WARN')
# sc = spark.sparkContext

# add more functions as necessary

def main(inputs, output):
    # main logic starts here

if __name__ == '__main__':
    inputs = sys.argv[1]
    output = sys.argv[2]
    main(inputs, output)
```

### reduce strings

```
@functions.udf(returnType=types.StringType())
def to_name_str(namelist):
    return ', '.join(sorted(namelist))


res_df = res_df.groupby('orderkey', 'totalprice')\
        .agg(to_name_str(functions.collect_set('name')).alias('name'))
```


### udf

`filename` is a column.

```
@functions.udf(returnType=types.StringType())
def path_to_hour(path):
    return re.search('\d{8}-\d{2}', path).group(0)

wikipedia = spark.read.csv(inputs, schema=wikipedia_schema, sep=' ')\
    .withColumn('filename', functions.input_file_name())\
    .withColumn('hour', path_to_hour('filename'))
```

### list to dataframe pyspark

```
orderkeys_df = spark.createDataFrame(orderkeys, IntegerType())
```

### spark config

```
spark = SparkSession.builder.appName('TPCH Denormalize') \
    .config('spark.cassandra.connection.host', ','.join(cluster_seeds))\
    .config('spark.dynamicAllocation.maxExecutors', 16)\
    .getOrCreate()
```

### Get the value in one row one column dataframe

```
r = res.collect()[0][0]
```

### combine two dataframe

```
result = slope.union(intercept)

```

### RDD to DataFrame: createDataFrame

```
line_re = re.compile(r'^(\S+) - - \[(\S+) [+-]\d+\] \"[A-Z]+ (\S+) HTTP/\d\.\d\" \d+ (\d+)$')
logs = sc.textFile(input_dir) \
    .repartition(10) \
    .map(lambda x: line_re.split(x)) \
    .map(format_line) \
    .filter(lambda x: x is not None)
data_rows = logs.map(lambda x: Row(host=x[0], id=str(uuid.uuid4()), bytes=x[1], datetime=x[2], path=x[3]))
data = spark.createDataFrame(data_rows)
```

### Create dataframe from scratch

#### Multiple columns

```
test = spark.createDataFrame([(1, 1), (1, 2), (1, 3)], ["user_id", "business_id"])
```

#### Single Column

```
user_id = 1
user_df = spark.createDataFrame([user_id], types.IntegerType())
user_df = user_df.select(user_df['value'].alias('user_id'))
```

### Extract Rows inside one grid of dataframe using `.flatMap`

```
rec = ratings_model.recommendForUserSubset(user_df, 3).select('recommendations')
```

```
>>> rec.show()
+-------+--------------------+                                                  
|user_id|     recommendations|
+-------+--------------------+
|      1|[[16356, 9.150447...|
+-------+--------------------+
```

```
rec_rdd = rec.rdd\
    .flatMap(lambda x: x['recommendations'])\
    .map(lambda x: (x['business_id'], x['rating']))
```

### withColumn

```
data = data.withColumn('slope',
        (data['sum((x * y))']-data['sum(x)']*data['sum(y)']/data['sum(1)']) / (data['sum(POWER(x, 2))']-data['sum(x)']**2/data['sum(1)']))
```

### write to json

```
resDF.write.json(outputDir, compression='gzip', mode='overwrite')
```

### write to parquet

```
newDF1.write.parquet('newDF1')
```

### write to csv

```
averagesDF.write.csv(outputDir, mode='overwrite')
```

### write to a single csv

```
facultyDF.repartition(1).write.format('com.databricks.spark.csv').option('header', 'true').save('faculty_table.csv')
```

Without folder:

```
facultyDF.toPandas().to_csv("faculty_table.csv", header=True)
```

### concat two column

Concat column cols[0] and cols[1] with a whitespace as separator.

```
df = df.withColumn('joinKey', functions.concat(cols[0], functions.lit(' '), cols[1]))
```


### dataframe show full column

Show `100` rows by `100` and full column content by `False`

```
df.select('joinKey').show(100, False)
```

### remove duplicate rows

`drop_duplicates()` is an alias for `dropDuplicates()`.

```
df = df.dropDuplicates()
```

```
>>> from pyspark.sql import Row
>>> df = sc.parallelize([ \
...     Row(name='Alice', age=5, height=80), \
...     Row(name='Alice', age=5, height=80), \
...     Row(name='Alice', age=10, height=80)]).toDF()
>>> df.dropDuplicates().show()
+---+------+-----+
|age|height| name|
+---+------+-----+
|  5|    80|Alice|
| 10|    80|Alice|
+---+------+-----+
>>> df.dropDuplicates(['name', 'height']).show()
+---+------+-----+
|age|height| name|
+---+------+-----+
|  5|    80|Alice|
+---+------+-----+
```

### split column into rows

`explode` split each element in array of specified column.

dtype of joinKey column is `array<string>`, like `[adobe, indesign, cs3, mac, upgrade, pagemaker]`

The new column containing splitted element is `word`.

```
df1 = df1.withColumn('word', functions.explode(functions.col('joinKey')))
```

Another example in scale.

```
scala> movies.show(truncate = false)
+-------+---------+-----------------------+
|movieId|movieName|genre                  |
+-------+---------+-----------------------+
|1      |example1 |action|thriller|romance|
|2      |example2 |fantastic|action       |
+-------+---------+-----------------------+

scala> movies.withColumn("genre", explode(split($"genre", "[|]"))).show
+-------+---------+---------+
|movieId|movieName|    genre|
+-------+---------+---------+
|      1| example1|   action|
|      1| example1| thriller|
|      1| example1|  romance|
|      2| example2|fantastic|
|      2| example2|   action|
+-------+---------+---------+

// You can use \\| for split instead
scala> movies.withColumn("genre", explode(split($"genre", "\\|"))).show
+-------+---------+---------+
|movieId|movieName|    genre|
+-------+---------+---------+
|      1| example1|   action|
|      1| example1| thriller|
|      1| example1|  romance|
|      2| example2|fantastic|
|      2| example2|   action|
+-------+---------+---------+
```

### column types

```
df.dtypes
```

### many lists to dataframe

```
flat_lats = [float(item) for sublist in lats for item in sublist]
flat_lons = [float(item) for sublist in lons for item in sublist]
flat_elevs = [float(item) for sublist in elevs for item in sublist]

tupleList = list(zip(flat_lats, flat_lons, flat_elevs))

df = spark.createDataFrame(tupleList, ['latitude', 'longitude', 'elevation'])\
    .withColumn('date', functions.lit(selected_date))
```

### sql compare year of a date

```
middle_year = 1900
sql = 'select t1.longitude, t1.latitude, t2.avg ' \
       'from weather t1, (select station, avg(tmax) as avg, count(*) ' \
       'from weather ' \
       'where year(date)<{} ' \
       'group by station) t2 ' \
       'where t1.station=t2.station'.format(middle_year)
w_df = spark.sql(sql)
```

### dataframe column to list

```
df.select(column_name).rdd.map(lambda r: r[column_name]).collect()
```

## Reference

[Pyspark 2.3.1 document](https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark.sql.functions.array)