---
layout: post
author: Hang Hu
categories: mysql
tags: Blog Database Mysql 
cover: 
---

## Details

When I use pySpark to write dataframe to MySQL, it always appears this exception.

## Reason

When business_id is 0, since MySQL treats 0 specially as Null, it will insert as a random business_id like 94157, which conflicts the row with business_id 94157.

`zipWithIndex()` will index from 0.

## Solution

Make business_id start from 1 by `.zipWithIndex().map(lambda x: (x[0], x[1]+1))`.

```
business_dict = business_rdd.map(lambda x: x[0]).distinct().zipWithIndex().map(lambda x: (x[0], x[1]+1)).collectAsMap()
```
