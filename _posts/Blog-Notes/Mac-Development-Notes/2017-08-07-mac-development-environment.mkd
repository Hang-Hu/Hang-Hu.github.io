---
layout: post
author: Hang Hu
categories: mac-development
tags: Mac-Development 
cover: 
---

```
brew install mysql
brew services start mysql or mysql.server start
mysqladmin -u root password // set password

brew cask install java
brew install wget
brew install curl
brew install maven
brew install python3
brew install git
brew install scala
brew cask install docker
```


```
brew install p7zip
brew cask install squirrel
brew cask install iterm2
```

Install `pip`:

```
sudo easy_install pip
```

Or

```
curl -O https://bootstrap.pypa.io/get-pip.py
sudo python get-pip.py
```

Don't use `brew cask install sublime-text`, download it directly from the website and install.

## Check the availability


```
ls -lrt /usr/local/bin
```



```
echo $HADOOP_HOME
```


## Scala


```
sudo vim /etc/profiles
export SCALA_HOME=/usr/local/Cellar/scala
export PATH=$PATH:$SCALA_HOME/bin
```


## Hadoop


```
sudo vim /etc/profiles
```


Add the following line:


```
# hadoop

export HADOOP_HOME=/Users/hanhu/Documents/Programs/hadoop-2.6.0
```


### configure environment variables


```
sudo vim /etc/profile
export JAVA_HOME=/usr/lib/jvm/default-java
export HADOOP_HOME=/apache/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

source /etc/profile
```


### create directories


```
//if /data exists, no need to create again
sudo mkdir /data

//create other directories
sudo chmod 777 /data
mkdir -p /data/hadoop-data/nn
mkdir -p /data/hadoop-data/snn
mkdir -p /data/hadoop-data/dn
mkdir -p /data/hadoop-data/tmp
mkdir -p /data/hadoop-data/mapred/system
mkdir -p /data/hadoop-data/mapred/local

mkdir -p /tmp/logs
sudo chmod 777 /tmp/logs
```


### configure config files


```
cd /apache/hadoop/etc/hadoop

//hadoop-env.sh     (vi hadoop-env.sh)
# export JAVA_HOME=${JAVA_HOME}

export JAVA_HOME=/usr/lib/jvm/default-java

//hdfs-site.xml     (vi hdfs-site.xml)
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.name.dir</name>
        <value>file:///data/hadoop-data/nn</value>
    </property>
    <property>
        <name>dfs.data.dir</name>
        <value>file:///data/hadoop-data/dn</value>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.dir</name>
        <value>file:///data/hadoop-data/snn</value>
    </property>
    <property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
    </property>
</configuration>

//core-site.xml     (vi core-site.xml)
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:///data/hadoop-data/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
</configuration>

cp mapred-site.xml.template mapred-site.xml

//mapred-site.xml       (vi mapred-site.xml)
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address </name>
        <value>localhost:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>localhost:19888</value>
    </property>
</configuration>

//yarn-site.xml     (vi yarn-site.xml)
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>localhost</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.nodemanager.log-dirs</name>
        <value>/tmp/logs</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>/yarn-logs/logs</value>
    </property>
    <property>
        <name>yarn.nodemanager.remote-app-log-dir-suffix</name>
        <value>logs</value>
    </property>
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>360000</value>
    </property>
    <property>
        <name>yarn.log.server.url</name>
        <value>http://localhost:19888/jobhistory/logs</value>
    </property>
    
    <!-- for java 8 -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
    
</configuration>

//slaves
localhost
```


### Set ssh


```
ssh-keygen -t rsa
cat id_rsa.pub >>  authorized_keys
chmod og-wx authorized_keys
```


### Start hadoop


```
hdfs namenode -format
start-dfs.sh
sudo hdfs dfs -mkdir /user
sudo hdfs dfs -mkdir /user/hanhu
start-yarn.sh sudo
mr-jobhistory-daemon.sh start historyserver

//now you can visit 10.149.252.77:8088 to see yarn services status
```


## Hive


```
sudo vim /etc/profile
```


Add the following lines:


```
# hive

export HIVE_HOME=/Users/hanhu/Downloads/apache-hive-1.2.2-bin
export PATH=$PATH:$HIVE_HOME/bin
export HADOOP_USER_CLASSPATH_FIRST=true
```


```
vim $HIVE_HOME/bin/hive-config.sh
```


Check the first two lines and add the `HADOOP_HOME`.


```
 export HIVE_CONF_DIR=$HIVE_CONF_DIR
 export HIVE_AUX_JARS_PATH=$HIVE_AUX_JARS_PATH
 export HADOOP_HOME=/Users/hanhu/Downloads/hadoop-2.6.5
```


```
cd $HIVE_HOME/conf
cp hive-default.xml.template hive-site.xml
vi hive-site.xml

//hive-site.xml
<property> 
  <name>hive.execution.engine</name>
  <value>mr</value>
</property>

<property>
    <name>hive.exec.scratchdir</name>
    <value>/tmp/hive-${user.name}</value>
  </property>
  <property>
    <name>hive.exec.local.scratchdir</name>
    <value>/tmp/${user.name}</value>
  </property>
  <property>
    <name>hive.downloaded.resources.dir</name>
    <value>/tmp/${user.name}_resources</value>
  </property>
  <property>
    <name>hive.scratch.dir.permission</name>
    <value>733</value>
</property>
```



### Configure mysql


Create database


```
CREATE DATABASE metastore;
USE metastore;
SOURCE /Users/hanhu/Downloads/apache-hive-1.2.2-bin/scripts/metastore/upgrade/mysql/hive-schema-1.2.0.mysql.sql;
```


Create user, 'localhost' means the metastore service is on localhost, '123456' means the user 'hive' has password '123456'.


```
CREATE USER 'hive'@'localhost' IDENTIFIED BY '123456';
REVOKE ALL PRIVILEGES, GRANT OPTION FROM 'hive'@'localhost';
GRANT SELECT,INSERT,UPDATE,DELETE,LOCK TABLES,EXECUTE ON metastore.* TO 'hive'@'localhost';
FLUSH PRIVILEGES;
```


### configure metastore service to communicate with the mysql database


```
cp /Users/hanhu/.m2/repository/mysql/mysql-connector-java/5.1.40/mysql-connector-java-5.1.40.jar $HIVE_HOME/lib/
```


```
cd /apache/hive/conf
vi hive-site.xml

//hive-site.xml         //on slave nodes, modify localhost to the mysql service node ip (here is master node)
//"localhost" should not be modified to ip address, it will fail to connect mysql service
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost:3306/metastore?createDatabaseIfNotExist=true</value>
  <description>the URL of the MySQL database</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>123456</value>
</property>

<property>
  <name>datanucleus.autoCreateSchema</name>
  <value>false</value>
</property>

<property>
  <name>datanucleus.fixedDatastore</name>
  <value>true</value>
</property>

<property>
  <name>hive.metastore.uris</name>
  <value>thrift://localhost:9083</value>
  <description>IP address (or fully-qualified domain name) and port of the metastore host</description>
</property>
```

### Create subl command to open sublime text 3

```
ln -sv "/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl" /usr/local/bin/subl
```
