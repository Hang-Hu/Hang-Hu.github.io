---
layout: post
author: Hang Hu
categories: hadoop
tags: Blog Java Hadoop 
cover: 
---

```
yarn application -list
```


And kill a specific job:



```
yarn application -kill <APPLICATION_ID>
```


If you have unnecessary files sitting around (especially large files created as part of an assignment), please clean them up with a command like this:


```
hdfs dfs -rm -r /user/<USERID>/output*
```


You will also frequently need to copy files to the cluster:


```
[yourcomputer]$ scp assignment.jar <USERID>@gateway.sfucloud.ca:
```


The hdfs dfs command (which is a synonym for hadoop fs commands, which you might also see in docs) is used to interact with the file system. By convention, your home directory is /user/<USERID>/ and you should keep your files in there. For example, on the cluster, either of these commands will list the files in your home directory:


```
hdfs dfs -ls /user/<USERID>
hdfs dfs -ls
```


Create a directory to hold input files for our first job and copy some files into it. There are some files to work with on the gateway node in /home/bigdata/. (These are the texts from the NLTK Gutenberg corpus plus one extra that we'll see later.) It is much easier to separate the files of a data set into their own directory: the easiest way to specify input is “all of the files in this directory.”


`copyFromLocal` is copying from `ext4` file to hdfs.


```
hdfs dfs -mkdir wordcount-2
hdfs dfs -copyFromLocal /home/bigdata/wordcount/* wordcount-2/
```


This is the normal copy, from `hdfs` to `hdfs`.


```
hdfs dfs -mkdir wordcount-1
hdfs dfs -cp wordcount-2/a* wordcount-1/
```


## Compile jar


```
javac -classpath `${HADOOP_HOME}/bin/hadoop classpath` WordCount.java

jar cf wordcount.jar WordCount*.class
```


## Upload


```
scp wordcount.jar SFU:
```


## Run WordCount


The default is one reducer:


```
yarn jar wordcount.jar WordCount wordcount-1 output-1
```


You can use three reducers:


```
yarn jar wordcount.jar WordCount -D mapreduce.job.reduces=3 \
    wordcount-1 output-2
```


(or equivalently, could do `job.setNumReduceTasks(3)` in the run method.) Re-run the job with three reducers and have a look at the output. [❓]


You can also specify zero reducers: in that case, Hadoop will simply dump the output from the mappers. This of this as a chance to debug the intermediate output. Try that:


```
yarn jar wordcount.jar WordCount -D mapreduce.job.reduces=0 \
    wordcount-1 output-3
```




## Check the result


```
hdfs dfs -ls output-1
hdfs dfs -cat output-1/part-r-00000 | less
```


And remove it if you want to run again:


```
hdfs dfs -rm -r output-1
```


Find words starting with 'better' by `grep -i "^better"`:


```
hdfs dfs -cat output-1/part-r-00000 | grep -i "^better"
```

